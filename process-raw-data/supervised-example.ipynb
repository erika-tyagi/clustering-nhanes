{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NHANES survey includes a substantive nutritional component that asks individuals to record what they ate over two days. This yielded a set of ~120 features corresponding to the amount of various nutrients consumed across each of these days. \n",
    "\n",
    "Unsupervised approach –  \n",
    "1. Limited sample to adults who completed both days of the survey, and who reported both days as reflecting their usual food consumption patterns. \n",
    "2. Summed nutrientional intake across the two days for each individual (reducing our feature space to ~ 60). \n",
    "3. Used PCA to reduce set of 60 nutrients to a lower-dimensional feature space. \n",
    "4. Used this reduced set of components to cluster individuals based on their nutritional profile. \n",
    "\n",
    "Supervised extension –\n",
    "\n",
    "There's a vast literature on the social determinants of health that explores how demographic factors (e.g., race, income, etc.) affect health outcomes. We wanted to see how nutritional intake plays into this (specifically given the increasing attention given to disparities in food access / food sarcity). Specifically, we ran models predicting obesity and high blood pressure using just demographic factors. We then also included the nutritional profiles gleaned from above to see how this affects predictive performance. Specifically we were interested in how meaningful the clusters were as predictors, and how meaningful the components were as predictors. \n",
    "\n",
    "\n",
    "Other thought – \n",
    "\n",
    "Maybe instead of supervised ML, we should use regression analysis (to see if the cluster assignment / components are statistically significant predictors after controlling for demographic variables)? Effectively, this would be how to disentangle the issue of demographic factors affecting the outcome, but also affecting the clusters (which you mentioned earlier). Given that the nutritional features don't seem to add much, I wonder if this is what's going on anyways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>SEQN</th>\n",
       "      <th>BMXBMI</th>\n",
       "      <th>BPQ020</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>INDFMPIR</th>\n",
       "      <th>RIDRETH1</th>\n",
       "      <th>TKCAL</th>\n",
       "      <th>TPROT</th>\n",
       "      <th>TCARB</th>\n",
       "      <th>TSUGR</th>\n",
       "      <th>TTFAT</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>PC11</th>\n",
       "      <th>PC12</th>\n",
       "      <th>PC13</th>\n",
       "      <th>PC14</th>\n",
       "      <th>PC15</th>\n",
       "      <th>PC16</th>\n",
       "      <th>PC17</th>\n",
       "      <th>PC18</th>\n",
       "      <th>PC19</th>\n",
       "      <th>PC20</th>\n",
       "      <th>assignment_kmeans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-2008</td>\n",
       "      <td>41475</td>\n",
       "      <td>58.04</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>1.83</td>\n",
       "      <td>5</td>\n",
       "      <td>3057</td>\n",
       "      <td>139.31</td>\n",
       "      <td>348.69</td>\n",
       "      <td>160.48</td>\n",
       "      <td>125.33</td>\n",
       "      <td>-1.617823</td>\n",
       "      <td>1.919746</td>\n",
       "      <td>0.455475</td>\n",
       "      <td>-0.172202</td>\n",
       "      <td>0.297086</td>\n",
       "      <td>2.514255</td>\n",
       "      <td>-0.836439</td>\n",
       "      <td>-1.447478</td>\n",
       "      <td>1.210543</td>\n",
       "      <td>0.014087</td>\n",
       "      <td>3.585609</td>\n",
       "      <td>-1.442924</td>\n",
       "      <td>1.064670</td>\n",
       "      <td>-0.591428</td>\n",
       "      <td>1.113903</td>\n",
       "      <td>1.035296</td>\n",
       "      <td>-0.094012</td>\n",
       "      <td>0.249038</td>\n",
       "      <td>0.889505</td>\n",
       "      <td>-1.001357</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-2008</td>\n",
       "      <td>41479</td>\n",
       "      <td>27.56</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1</td>\n",
       "      <td>4525</td>\n",
       "      <td>215.80</td>\n",
       "      <td>531.05</td>\n",
       "      <td>197.91</td>\n",
       "      <td>179.99</td>\n",
       "      <td>1.190533</td>\n",
       "      <td>-1.640098</td>\n",
       "      <td>-2.723926</td>\n",
       "      <td>0.382880</td>\n",
       "      <td>0.345203</td>\n",
       "      <td>-0.298905</td>\n",
       "      <td>-0.327906</td>\n",
       "      <td>0.688157</td>\n",
       "      <td>-1.332718</td>\n",
       "      <td>0.432461</td>\n",
       "      <td>-1.024977</td>\n",
       "      <td>0.579877</td>\n",
       "      <td>1.673101</td>\n",
       "      <td>-0.458015</td>\n",
       "      <td>-0.004225</td>\n",
       "      <td>0.127652</td>\n",
       "      <td>-0.558152</td>\n",
       "      <td>-0.262917</td>\n",
       "      <td>-1.027508</td>\n",
       "      <td>-0.741834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-2008</td>\n",
       "      <td>41483</td>\n",
       "      <td>44.06</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>1.14</td>\n",
       "      <td>4</td>\n",
       "      <td>5620</td>\n",
       "      <td>205.34</td>\n",
       "      <td>758.05</td>\n",
       "      <td>207.17</td>\n",
       "      <td>219.10</td>\n",
       "      <td>4.022577</td>\n",
       "      <td>-1.276317</td>\n",
       "      <td>0.697245</td>\n",
       "      <td>1.035136</td>\n",
       "      <td>-2.475559</td>\n",
       "      <td>-1.047524</td>\n",
       "      <td>-0.086807</td>\n",
       "      <td>-0.030414</td>\n",
       "      <td>-0.990138</td>\n",
       "      <td>1.059824</td>\n",
       "      <td>2.126264</td>\n",
       "      <td>-1.084899</td>\n",
       "      <td>0.479882</td>\n",
       "      <td>-0.887813</td>\n",
       "      <td>-0.532492</td>\n",
       "      <td>-0.129027</td>\n",
       "      <td>-1.944706</td>\n",
       "      <td>-0.377889</td>\n",
       "      <td>-2.218713</td>\n",
       "      <td>-0.895268</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-2008</td>\n",
       "      <td>41485</td>\n",
       "      <td>25.99</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2</td>\n",
       "      <td>3118</td>\n",
       "      <td>111.14</td>\n",
       "      <td>315.96</td>\n",
       "      <td>127.14</td>\n",
       "      <td>157.96</td>\n",
       "      <td>-3.524582</td>\n",
       "      <td>2.625010</td>\n",
       "      <td>-0.403105</td>\n",
       "      <td>0.151470</td>\n",
       "      <td>-0.711641</td>\n",
       "      <td>0.225901</td>\n",
       "      <td>0.996899</td>\n",
       "      <td>0.639086</td>\n",
       "      <td>-0.611923</td>\n",
       "      <td>-0.044899</td>\n",
       "      <td>-0.729598</td>\n",
       "      <td>0.503148</td>\n",
       "      <td>-0.383034</td>\n",
       "      <td>0.470820</td>\n",
       "      <td>0.336835</td>\n",
       "      <td>0.293946</td>\n",
       "      <td>-0.393011</td>\n",
       "      <td>-0.438882</td>\n",
       "      <td>-0.431600</td>\n",
       "      <td>-0.221301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-2008</td>\n",
       "      <td>41486</td>\n",
       "      <td>31.21</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1</td>\n",
       "      <td>3171</td>\n",
       "      <td>87.98</td>\n",
       "      <td>558.07</td>\n",
       "      <td>279.28</td>\n",
       "      <td>78.93</td>\n",
       "      <td>-3.841056</td>\n",
       "      <td>-1.749876</td>\n",
       "      <td>1.175520</td>\n",
       "      <td>1.888064</td>\n",
       "      <td>0.689904</td>\n",
       "      <td>-0.950871</td>\n",
       "      <td>-0.905093</td>\n",
       "      <td>-0.764330</td>\n",
       "      <td>-0.566994</td>\n",
       "      <td>-1.560574</td>\n",
       "      <td>0.875038</td>\n",
       "      <td>-1.047852</td>\n",
       "      <td>-0.334103</td>\n",
       "      <td>0.376641</td>\n",
       "      <td>0.124650</td>\n",
       "      <td>-0.660536</td>\n",
       "      <td>-0.928691</td>\n",
       "      <td>0.317645</td>\n",
       "      <td>0.210854</td>\n",
       "      <td>-0.336989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        year   SEQN  BMXBMI  BPQ020  RIDAGEYR  RIAGENDR  INDFMPIR  RIDRETH1  \\\n",
       "0  2007-2008  41475   58.04       1        62         2      1.83         5   \n",
       "1  2007-2008  41479   27.56       2        52         1      2.20         1   \n",
       "2  2007-2008  41483   44.06       1        66         1      1.14         4   \n",
       "3  2007-2008  41485   25.99       2        30         2      1.01         2   \n",
       "4  2007-2008  41486   31.21       1        61         2      1.75         1   \n",
       "\n",
       "   TKCAL   TPROT   TCARB   TSUGR   TTFAT       PC1       PC2       PC3  \\\n",
       "0   3057  139.31  348.69  160.48  125.33 -1.617823  1.919746  0.455475   \n",
       "1   4525  215.80  531.05  197.91  179.99  1.190533 -1.640098 -2.723926   \n",
       "2   5620  205.34  758.05  207.17  219.10  4.022577 -1.276317  0.697245   \n",
       "3   3118  111.14  315.96  127.14  157.96 -3.524582  2.625010 -0.403105   \n",
       "4   3171   87.98  558.07  279.28   78.93 -3.841056 -1.749876  1.175520   \n",
       "\n",
       "        PC4       PC5       PC6       PC7       PC8       PC9      PC10  \\\n",
       "0 -0.172202  0.297086  2.514255 -0.836439 -1.447478  1.210543  0.014087   \n",
       "1  0.382880  0.345203 -0.298905 -0.327906  0.688157 -1.332718  0.432461   \n",
       "2  1.035136 -2.475559 -1.047524 -0.086807 -0.030414 -0.990138  1.059824   \n",
       "3  0.151470 -0.711641  0.225901  0.996899  0.639086 -0.611923 -0.044899   \n",
       "4  1.888064  0.689904 -0.950871 -0.905093 -0.764330 -0.566994 -1.560574   \n",
       "\n",
       "       PC11      PC12      PC13      PC14      PC15      PC16      PC17  \\\n",
       "0  3.585609 -1.442924  1.064670 -0.591428  1.113903  1.035296 -0.094012   \n",
       "1 -1.024977  0.579877  1.673101 -0.458015 -0.004225  0.127652 -0.558152   \n",
       "2  2.126264 -1.084899  0.479882 -0.887813 -0.532492 -0.129027 -1.944706   \n",
       "3 -0.729598  0.503148 -0.383034  0.470820  0.336835  0.293946 -0.393011   \n",
       "4  0.875038 -1.047852 -0.334103  0.376641  0.124650 -0.660536 -0.928691   \n",
       "\n",
       "       PC18      PC19      PC20  assignment_kmeans  \n",
       "0  0.249038  0.889505 -1.001357                  1  \n",
       "1 -0.262917 -1.027508 -0.741834                  1  \n",
       "2 -0.377889 -2.218713 -0.895268                  2  \n",
       "3 -0.438882 -0.431600 -0.221301                  1  \n",
       "4  0.317645  0.210854 -0.336989                  1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clustered_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data \n",
    "df['is_obese'] = np.where(df['BMXBMI'] > 30, 1, 0)\n",
    "df['has_high_bp'] = np.where(df['BPQ020'] == 1, 1, 0)\n",
    "df['male'] = np.where(df['RIAGENDR'] == 1, 1, 0)\n",
    "df['white'] = np.where(df['RIDRETH1'] == 3, 1, 0)\n",
    "df['black'] = np.where(df['RIDRETH1'] == 4, 1, 0)\n",
    "df['hispanic'] = np.where(df['RIDRETH1'] < 3, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_df, test_df, label, feat, t): \n",
    "    train_X, test_X = train_df[feat], test_df[feat]\n",
    "    train_y, test_y = train_df[label].ravel(), test_df[label].ravel()\n",
    "    model = RandomForestClassifier(n_estimators = 100, random_state = 123)\n",
    "\n",
    "    clf = model.fit(train_X, train_y)\n",
    "    test_prob = clf.predict_proba(test_X)[:,1]\n",
    "    pred = np.where(test_prob > t, 1, 0)\n",
    "    \n",
    "    metrics = (accuracy_score(test_y, pred), \n",
    "               precision_score(test_y, pred), \n",
    "               recall_score(test_y, pred)) \n",
    "\n",
    "    feat_imp = pd.DataFrame(clf.feature_importances_,\n",
    "                            index = train_X.columns,\n",
    "                            columns=['Importance']).sort_values('Importance', ascending = False)\n",
    "    \n",
    "    return metrics, feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6308805790108565, 0.4597156398104265, 0.16302521008403362)\n",
      "          Importance\n",
      "INDFMPIR    0.608746\n",
      "RIDAGEYR    0.341542\n",
      "male        0.014293\n",
      "black       0.013071\n",
      "white       0.012282\n",
      "hispanic    0.010066\n"
     ]
    }
   ],
   "source": [
    "# just demographic features \n",
    "train_df, test_df = train_test_split(df)\n",
    "label = 'is_obese'\n",
    "feat = ['male', 'RIDAGEYR', 'white', 'black', 'hispanic', 'INDFMPIR']\n",
    "\n",
    "metrics, feat_imp = run_model(train_df, test_df, label, feat, 0.7)\n",
    "print(metrics)\n",
    "print(feat_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6387213510253317, 0.49473684210526314, 0.03926482873851295)\n",
      "                   Importance\n",
      "RIDAGEYR             0.800308\n",
      "black                0.054521\n",
      "white                0.044602\n",
      "hispanic             0.040985\n",
      "male                 0.034640\n",
      "assignment_kmeans    0.024943\n"
     ]
    }
   ],
   "source": [
    "# demographic features and cluster \n",
    "train_df, test_df = train_test_split(df)\n",
    "label = 'is_obese'\n",
    "feat = ['male', 'RIDAGEYR', 'white', 'black', 'hispanic', 'assignment_kmeans']\n",
    "t = 0.7\n",
    "\n",
    "metrics, feat_imp = run_model(train_df, test_df, label, feat, 0.7)\n",
    "print(metrics)\n",
    "print(feat_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6429433051869723, 0.5454545454545454, 0.005063291139240506)\n",
      "          Importance\n",
      "PC2         0.151047\n",
      "PC5         0.140033\n",
      "PC1         0.139400\n",
      "PC3         0.138091\n",
      "PC4         0.136697\n",
      "RIDAGEYR    0.122246\n",
      "INDFMPIR    0.119378\n",
      "male        0.016415\n",
      "white       0.012801\n",
      "hispanic    0.012375\n",
      "black       0.011518\n"
     ]
    }
   ],
   "source": [
    "# demographic features and components \n",
    "train_df, test_df = train_test_split(df)\n",
    "label = 'is_obese'\n",
    "feat = ['male', 'RIDAGEYR', 'white', 'black', 'hispanic', 'INDFMPIR', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5']\n",
    "t = 0.7\n",
    "\n",
    "metrics, feat_imp = run_model(train_df, test_df, label, feat, 0.7)\n",
    "print(metrics)\n",
    "print(feat_imp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
